---
title: "MAT 400 Final Project"
author: "Aidan and Liam Greenlee"
subtitle: NBA Shot Prediction
---

```{r markdown, include=FALSE}
library(tidyverse)
require(gridExtra)
library(tidymodels)
library(dplyr)
library(e1071)
library(modelr)
library(caret)
library(lubridate)
library(tree)
library(glmnet)
```

## DATA EXPLORATION
```{r}
data <- na.omit(read.csv(file = "data.csv"))
data <- mutate(data,time_remaining = 60*minutes_remaining + seconds_remaining)
```
Here we are cleaning the data a little, combining minutes_remaining and seconds_remaining into one variable as well as omitting NA values.

```{r}
ggplot(data, aes(x=shot_distance, y = time_remaining,color=as_factor(shot_made_flag)))+geom_point()
```
It seems like shot_distance is related to shot_made_flag

```{r, echo=FALSE}
ggplot(data, aes(x=combined_shot_type,y = as_factor(shot_made_flag))) + geom_jitter()
```
This jitter plot shows that some shot types can be used t determine if the shot was made or missed. (dunks are usually made)

```{r, echo=FALSE}
ggplot(data, aes(x = lat, y = lon, color = shot_zone_area)) + geom_point() + coord_fixed(ratio = 1)# + theme(legend.position = "none")

ggplot(data, aes(x = lat, y = lon, color = as.factor(shot_made_flag))) + geom_point() + coord_fixed(ratio = 1)# + theme(legend.position = "none")
```
It seems like shots from the back of the court are almost always missed. This is could be a good predictor

```{r, echo=FALSE}
ggplot(data, aes(x=loc_x,y=loc_y, color = shot_distance)) + geom_point() + coord_fixed(ratio = 1) + scale_colour_gradientn(colours=rainbow(100))
```

The exploration indicates thate the best predictors could be shot distance, combined shot type, and maybe shot zone area.

## SPLITTING DATA
```{r split, results='hide'}

#data$shot_made_flag <- as.factor(data$shot_made_flag)
dataUsed <- data[,c(2,9,6,7,10,14,15,16,17,18,24,26)]
resample <- initial_split(prop = .7, dataUsed)
test <- testing(resample)
train <- training(resample)
```

The data is split into test and train sets.
We omitted some columns that include information from other columns or were not important (game id, shot id, etc.)

## XGBOOST
```{r, echo=FALSE}
library(xgboost)

priorProbability <- sum(data$shot_made_flag)/nrow(data)
```


```{r}
nfold <- 10
m <- model.matrix(shot_made_flag~., data = train)
m2 <- model.matrix(shot_made_flag~., data = test)
```

An xgboost model is used with 10 fold cross validation.

```{r}
xgboostmodel <- xgb.cv(data = m, prediction = TRUE, callbacks = list(cb.cv.predict(save_models = TRUE)), label = train$shot_made_flag, nfold = nfold, nrounds = 1000, verbose = 1, objective = "binary:logistic", early_stopping_rounds = 10)
```

```{r}
predserr <- data.frame(numeric(nfold))
xgboostmodel
```


```{r, include=FALSE}
xgboostmodel$models
```

Here, we are performing 10 fold cross validation on xgboost to choose the best model.

```{r}
for(i in 1:nfold) { 
model <- xgb.Booster.complete(xgboostmodel$models[[i]])

test$pred <- predict(model, m2)
test <- test %>% mutate(prediction = ifelse(pred > (1-priorProbability), 1, 0))
test <- test %>% mutate(correct = ifelse(prediction == shot_made_flag, 1, 0))
predserr$numeric.nfold.[i] <- 1-sum(test$correct)/nrow(test)
}

n <- which.min(predserr$numeric.nfold.)
predserr
bestmodel <- xgb.Booster.complete(xgboostmodel$models[[n]])
predserr$numeric.nfold.[n]
```

The misclassification rate of this model is ~ 40%.
Some more models will be tested to achieve a lower misclassification rate.

## LOGISTIC REGRESSION
We decided to try logistic regression next.
```{r}
glm_fit <- glm(shot_made_flag ~ .,
    data = train,
    family = binomial
)
```

```{r}
testPred <- test
testPred <- testPred %>% add_predictions(glm_fit) %>% mutate(prob = exp(pred)/ (1 + exp(pred)))

testPred <- testPred %>% 
    mutate(shot_made = ifelse(prob > 0.5, 1, 0))

library(tidymodels)
autoplot(roc_curve(testPred, as.factor(shot_made_flag), prob))
roc_auc(testPred, as.factor(shot_made_flag), prob)

# tidyverse

testPred <- testPred %>% mutate(correct = ifelse(shot_made == shot_made_flag, 1, 0))
1-sum(testPred$correct)/nrow(testPred)
```
This model performed roughly the same as the xgboost model as shown by the ROC curve and the misclassification rate.

## LINEAR DISCRIMINANT ANALYSIS
We discovered the package caret to train data using a bunch of different models, so we decide to try a few out, starting with LDA.
```{r}
testPred <- test
ldaModel <- train(as.factor(shot_made_flag) ~ ., data=train, method="lda", trControl = trainControl(method = "cv", number = 5))

testPred <- testPred%>%add_predictions(ldaModel)

testPred <- testPred %>% mutate(correct = ifelse(pred == shot_made_flag, 1, 0))
1-sum(testPred$correct)/nrow(testPred)
```
## BAYES GLM
Next we decided to use Bayes GLM from caret.
```{r}
testPred <- test
bayesmod <- train(as.factor(shot_made_flag) ~ ., data=train, method="bayesglm", trControl = trainControl(method = "cv",number = 5))

testPred <- testPred%>%add_predictions(bayesmod)

testPred <- testPred %>% mutate(correct = ifelse(pred == shot_made_flag, 1, 0))
1-sum(testPred$correct)/nrow(testPred)
```
## NNET
We also decided to a neural network model from caret
```{r, results = 'hide'}
testPred <- test
nnetmod <- train(as.factor(shot_made_flag) ~ ., data=train, method="nnet", trControl = trainControl(method = "cv",number = 5))
```


```{r}
testPred <- testPred%>%add_predictions(nnetmod)

testPred <- testPred %>% mutate(correct = ifelse(pred == shot_made_flag, 1, 0))
1-sum(testPred$correct)/nrow(testPred)
```

## EARTH
Finally, we decided to use earth from caret.
```{r}
testPred <- test
earthmod <- train(as.factor(shot_made_flag) ~ ., data=train, method="earth", trControl = trainControl(method = "cv",number = 5))

testPred <- testPred%>%add_predictions(earthmod)

testPred <- testPred %>% mutate(correct = ifelse(pred == shot_made_flag, 1, 0))
1-sum(testPred$correct)/nrow(testPred)
```

We found that using all of these led to similar results, around a 39% misclassification rate


## TREE
We tried a tree model
```{r}
testPred <- test
train$shot_made_flag <- as_factor(train$shot_made_flag)
treefit <- tree(data = train, shot_made_flag ~ .  -opponent, method = "misclass")
plot(treefit)
text(treefit, all = TRUE, cex = .7)
```
This plot shows the tree. There are not many nodes, which we thought was unusual.

```{r}
fittree <- cv.tree(treefit, method = "misclass")
plot(fittree$size, fittree$dev, type = "b")
```

This graph shows the best size for the tree, which is used for the pruning.
```{r}
prune_fit <- prune.misclass(treefit, best = 3)
plot(prune_fit, type = "uniform")
text(prune_fit, all = TRUE, cex = .7)

testPred %>% modelr::add_predictions(prune_fit, type = "class") %>%
    accuracy(as.factor(shot_made_flag), pred)
```
The pruned tree looks the same as the original fit.
The tree model yields a similar misclassification rate to other models.